# GPT-from-scratch
A from scratch implementation of a GPT language model, trained on the TinyStories dataset with GPT-2 tokeniser. The goal of this project is to show how a small language model can still generate creative and coherent stories.

## Requirements
- Python 3.8+ 
- PyTorch 1.8+ 
- tiktoken 0.4 
- Datasets 1.6+ 

## Model Parameters
The model has ~89 million parameters

## Usage
To train the model, clone the repo and run the following command:

```bash
python gpt.py
```
You can change the dataset, hyperparameters, and save directory as you wish. once the training is completed, the model will generate 10000 new tokens. The generated text is printed and saved to the generated.txt file.

To generate stories, run the following command:

```bash
python generate.py
```
This will generate a sequence of 500 tokens from the model.

To print the tokens as the model generates, run the following command:

```bash
python sample.py
```

## Examples
Here are some examples of stories generated by the model:

- A boy named Tom wanted to be a superhero. He put on a cape and a mask and ran around the house. He pretended to fight bad guys and save people. He felt very happy and proud. His mom saw him and smiled. She said, "You are my superhero, Tom."
- A girl named Lily loved to draw. She had a lot of crayons and paper. She drew animals, flowers, stars, and anything she could imagine. She liked to share her drawings with her friends and family. They always praised her and encouraged her. She dreamed of becoming an artist one day.
- A dog named Max was very bored. He had no toys to play with and no one to play with him. He decided to go outside and look for some fun. He saw a cat sitting on a fence. He barked at it and chased it away. He felt very excited and happy. He thought, "This is fun."

## Model Architecture
The model is an autoregressive decoder-only Transformer language model which consists of the following components:

- An embedding layer that maps each token to a vector of size `n_embd`.
- A positional encoding layer that adds sinusoidal signals to the embeddings based on their positions in the sequence.
- A stack of `n_layer` transformer blocks, each containing:
    - A multi-head self-attention layer with `n_head` heads and `n_embd // n_head` head size.
    - A feed-forward layer with `4 * n_embd` hidden size and ReLU activation.
    - residual connections after each sub-layer with layer normalization layers placed according to pre-norm formulation.
    - Dropout with probability `dropout` after each sub-layer.
- A linear layer that projects the final hidden states to logits over the vocabulary.

The model uses causal masking in the self-attention layer to prevent the model from attending to future tokens.

## License

This project is licensed under the MIT License.

## References

- [TinyStories Dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
- [GPT-2 Tokenizer](https://github.com/openai/tiktoken)
- [attention-is-all-you-need](https://arxiv.org/abs/1706.03762)
- [ng-video-lecture](https://youtu.be/kCc8FmEb1nY?si=EAZ3np9Wdg2K17Fq)
- [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)
